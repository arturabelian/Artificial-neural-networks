# Нейросеть.

# Инициализация — задание количества входных, скрытых и выходных узлов.
# Тренировка — уточнение весовых коэффициентов в процессе обработки предоставленных для обучения сети тренировочных примеров.
# Опрос — получение значений сигналов с выходных узлов после предоставления значений входящих сигналов.

import numpy

# scipy.special для функции сигмойды expit()
import scipy.special

# Библиотека для отрисовки массивов.
import matplotlib.pyplot

# Для google colab.
%matplotlib inline

# Для загрузки данных из PNG файлов.
import imageio

# Определение класса нейросети.
class Neuralnetwork:

############################## ИНИЦИАЛИЗАЦИЯ ##############################

    # Инициализация нейросети.
    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):
        # Задаем кол-во узлов в слоях.
        self.inodes = inputnodes
        self.hnodes = hiddennodes
        self.onodes = outputnodes
        
        # Весовые матрицы связей, wih и who.
        
        # wih - Матрица весов (weight input-hidden) между входным слоем и скрытым.
        # who - Матрица весов (weight hidden-output)  между скрытым слоем и выходным.
         
        self.wih = numpy.random.normal(0.0, pow(self.inodes, -0.5), (self.hnodes, self.inodes))
        self.who = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.onodes, self.hnodes))

        # Рейт обучения.
        self.lr = learningrate
        
        # Сигмойда - функция активации используется через анонимную лямбда функцию.
        self.activation_function = lambda x: scipy.special.expit(x)
        
        pass

############################## ТРЕНИРОВКА ##############################
# Тренировка включает две фазы: первая — это расчет выходного сигнала, что и делает функция query(),
# а вторая — обратное распространение ошибок, информирующее о том, каковы должны быть поправки к весовым коэффициентам.

# Первая часть — расчет выходных сигналов для заданного тренировочного примера. Это ничем не отличается от того, что мы
# уже можем делать с помощью функции query().

# Вторая часть — сравнение рассчитанных выходных сигналов с желаемым ответом и обновление весовых коэффициентов связей
# между узлами на основе найденных различий.

# Единственным отличием является введение дополнительного параметра targets_list, передаваемого при вызове функции,
# поскольку невозможно тренировать сеть без предоставления ей тренировочных примеров, которые включают желаемые или целевые значения.

# Основной задачи тренировки сети — уточнению весов на основе расхождения между фактическими и целевыми значениями.

    # Тренировка нейросети.
    def train(self, inputs_list, targets_list):

        # Конвертирование списка входных сигналов в двумерную транспонированную матрицу.
        inputs = numpy.array(inputs_list, ndmin=2).T
        targets = numpy.array(targets_list, ndmin=2).T
        
        # Вычисление входящих сигналов для скрытого слоя.
        hidden_inputs = numpy.dot(self.wih, inputs)

        # Вычисление исходящих сигналов для скрытого слоя.
        hidden_outputs = self.activation_function(hidden_inputs)
        
        # Вычисление входящих сигналов для выходного слоя.
        final_inputs = numpy.dot(self.who, hidden_outputs)

        # Вычисление исходящих сигналов для выходного слоя.
        final_outputs = self.activation_function(final_inputs)

# Для весов связей между скрытым и выходным слоями используется output_errors.
     
        # Ошибка выходного слоя это (целевое значение - фактическое значение).
        output_errors = targets - final_outputs

# Далее мы должны рассчитать обратное распространение ошибок для узлов скрытого слоя.
# Для весов связей между входным и скрытым слоями hidden_errors.

        # Ошибка скрытого слоя hidden_errors это ошибки выходного слоя output_errors, распределенные пропорционально
        # весовым коэффициентам связи и рекомбинированные на скрытых узлах.

        hidden_errors = numpy.dot(self.who.T, output_errors) # Error_hide = W_ho.T * Error_o

# Обновление веса связи между двумя узлами.

# Величина а — это коэффициент обучения, а сигмоида — это функция активации. Последний член выражения — это
# транспонированная (т) матрица исходящих сигналов предыдущего слоя. В данном случае транспонирование означает
# преобразование столбца выходных сигналов в строку.
        
        # Обновление весов для связи между скрытым и выходным слоем.
        self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))
        
        # Обновление весов для связи между входным и скрытым слоем.
        self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))

# numpy.dot - Матричное умножение.

        pass

############################## ОПРОС ##############################
# Функция query() принимает в качестве аргумента входные данные нейронной сети и возвращает ее выходные данные.

# При этом, по мере распространения сигналов мы должны сглаживать их, используя весовые коэффициенты связей между
# соответствующими узлами, суммирование, а также применять сигмоиду для уменьшения выходных сигналов узлов.

# Получение входящих сигналов для узлов скрытого слоя путем сочетания матрицы весовых коэффициентов связей между
# входным и скрытым слоями с матрицей входных сигналов.

# X_скрытый = W_входной_скрытый * I_входные_сигналы

    # Опрос нейросети.
    def query(self, inputs_list):

        # Конвертирование входных сигналов в двумерную транспонированную матрицу.
        inputs = numpy.array(inputs_list, ndmin=2).T # 
        
        # Вычисление сингалов в скрытом слое.
        hidden_inputs = numpy.dot(self.wih, inputs) # HI = I * WIH

        # Вычисление сигналов выходящих из скрытого слоя.
        hidden_outputs = self.activation_function(hidden_inputs) # HO = SIG(HI)
        
        # Вычисление сигналов в конечно выходном слое.
        final_inputs = numpy.dot(self.who, hidden_outputs) # FI = HO * WHO

        # Вычисление сигналов выходящих из выходного слоя.
        final_outputs = self.activation_function(final_inputs) # FO = SIG(FI)
        
        return final_outputs


# Вместо того чтобы жестко задавать их в коде, мы предусмотрим установку соответствующих значений в виде
# параметров во время создания объекта нейронной сети. Благодаря этому можно будет без труда создавать новые
# нейронные сети различного размера.

# Мы хотим, чтобы один и тот же класс мог создавать как небольшие нейронные сети, так и очень большие, требуя
# лишь задания желаемых размеров сети в качестве параметров.

# Число входных, скрытых и выходных узлов.
input_nodes = 784
hidden_nodes = 200
output_nodes = 10

# Кроме того, нам нельзя забывать о рейте обучения.

# Рейт обучения 0.1
learning_rate = 0.1

# Создание экземпляра нейронной сети.
n = Neuralnetwork(input_nodes,hidden_nodes,output_nodes, learning_rate)

# Загрузка  набора тренировочных данных mnist из CSV файла.
training_data_file = open("mnist_dataset/mnist_train_100.csv", 'r')
training_data_list = training_data_file.readlines()
training_data_file.close()

# Трнировка нейросети.

# epochs - это количество тренировок.
epochs = 10

for e in range(epochs):
    # Проход через все записи в тренировочном наборе.
    for record in training_data_list:
        # Разделение записей ','.
        all_values = record.split(',')
        # Масштабирование и сдвиг входов.
        inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01
        # Создание нужного выходного значения (все 0.01, кроме желаемой метки 0.99)
        targets = numpy.zeros(output_nodes) + 0.01
        # all_values[0] нужная метка для этой записи.
        targets[int(all_values[0])] = 0.99
        n.train(inputs, targets)
        pass
    pass

# Тестирование нейросети с нашим изображением.

# Загрузка данных изображения из png файла в массив.
print ("loading ... my_image/my_image.png")
img_array = imageio.imread('my_image/my_image.png', as_gray=True)
    
# Изменение формы из 28x28 в список с 784 значениями.
img_data  = 255.0 - img_array.reshape(784)
    
# Масштабирование данных в диапазоне от 0.01 до 1.0
img_data = (img_data / 255.0 * 0.99) + 0.01
print("min = ", numpy.min(img_data))
print("max = ", numpy.max(img_data))

# Рисуем изображение.
matplotlib.pyplot.imshow(img_data.reshape(28,28), cmap='Greys', interpolation='None')

# Опрашиваем сеть.
outputs = n.query(img_data)
print (outputs)

# Индекс наивысшего значения передается метке.
label = numpy.argmax(outputs)
print("Network says ", label)

# loading ... my_image/my_image.png
# min =  0.01
# max =  1.0
# [[0.01619033]
#  [0.00805726]
#  [0.03006624]
#  [0.51209626]
#  [0.27860867]
#  [0.0466062 ]
#  [0.05860121]
#  [0.02263441]
#  [0.15948854]
#  [0.05217102]]
# Network says  3

# 51%
# На более сложном датасете показатель был равен 96%.